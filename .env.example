# OpenWebUI Configuration (optional)
OPENWEBUI_API_KEY=sk-abcdef123456
OPENWEBUI_URL=http://localhost:8080

# OpenAI-compatible endpoint (for Ollama or other providers)
OPENAI_ENDPOINT=http://localhost:11434/v1
OPENAI_API_KEY=ollama
OPENAI_VISION_MODEL=moondream

# Network Discovery Configuration (required)
DISCOVERY_CIDR_RANGES=192.168.1.0/24

# Network Discovery Configuration (optional - defaults shown)
DISCOVERY_INTERVAL_SECONDS=60
DISCOVERY_MAX_PARALLEL=10
DISCOVERY_TIMEOUT_SECONDS=2.0
DISCOVERY_PORT=11434

# Predefined Ollama Hosts (optional)
# Comma-separated list of static Ollama hosts in ip:port format
# These hosts are always visible with online/offline indicators and never removed
OLLAMA_HOSTS=192.168.1.10:11434,192.168.1.20:11434

# Endpoints Cache Refresh Interval (optional - default: 10 seconds)
# How often to refresh the /llmm endpoints cache to reduce load when multiple clients are polling
LLM_TRIGGER_ENDPOINTS_IN_SECONDS=10

# Logging Configuration (optional)
LOG_LEVEL=INFO

# Frontend Configuration
VITE_LLMMONITOR_URL=http://localhost:8000

# CORS Configuration (comma-separated list of allowed origins)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# LiteLLM Integration (optional)
LITELLM_URL=https://litellm-api.up.railway.app
LITELLM_MASTER_KEY=sk-your-master-key-here
